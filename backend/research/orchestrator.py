"""
Orchestrator Workflow for the Deep Research Application.
Manages the iterative research process, workers, and state aggregation.
"""

import os
import re
from typing import List, Optional, Union

from llama_index.core.workflow import (
    Workflow,
    Context,
    StartEvent,
    StopEvent,
    step,
)

from backend.research.state import ResearchState, WorkerState, Entity
from backend.research.events import (
    PlanCreatedEvent,
    WorkerStartEvent,
    WorkerResultEvent,
    IterationCompleteEvent,
    VerificationStartEvent,
    VerifyEntityEvent,
    VerificationResultEvent,
    GapFillEvent,
    GapFillingStartEvent,
    GapFilledEvent,
)
from backend.research import activities
from backend.research.logging_utils import get_session_logger


class DeepResearchWorkflow(Workflow):
    """
    Orchestrates the iterative research process.
    """

    @step
    async def start(self, ctx: Context, ev: StartEvent) -> PlanCreatedEvent:
        """
        Initializes the research state and generates the initial plan.
        """
        topic = ev.get("topic")
        if not topic:
            raise ValueError("Topic is required to start research.")

        # Initialize global state in Context
        state = ResearchState(topic=topic, status="running")
        # Save initial state to Context (and potentially DB)
        await ctx.store.set("state", state)

        # Setup session logger
        logger = get_session_logger(state.id)
        logger.info("Research session started for topic: %s", topic)

        # Log start
        state.logs.append("Workflow initialized.")
        await activities.save_state(state)

        # Generate Initial Plan (Agentic Step)
        plan = await activities.generate_initial_plan(topic, research_id=state.id)
        state.plan = plan
        state.logs.append(f"Plan generated: {plan.current_hypothesis}")

        # Initialize workers from plan
        for worker_cfg in plan.initial_workers:
            # Create WorkerState objects (ID preserved if provided, or generated by default)
            w_state = WorkerState(
                id=worker_cfg.worker_id,
                research_id=state.id,
                strategy=worker_cfg.strategy,
                queries=worker_cfg.example_queries,  # Pass actual queries
                status="ACTIVE",
            )
            state.workers[w_state.id] = w_state

        await activities.save_state(state)

        return PlanCreatedEvent(plan=plan)

    @step
    async def dispatch(
        self, ctx: Context, ev: Union[PlanCreatedEvent, IterationCompleteEvent]
    ) -> Optional[WorkerStartEvent]:
        """
        Fan-Out: Dispatches work to all active workers.
        """
        state: ResearchState = await ctx.store.get("state")

        # Identify active workers (including those categorized as productive or declining)
        active_workers = [
            w
            for w in state.workers.values()
            if w.status in ["ACTIVE", "PRODUCTIVE", "DECLINING"]
        ]

        if not active_workers:
            # No viable workers left to dispatch. Transition to verification.
            state.status = "verification_pending"
            state.logs.append("No active or productive workers remaining. Starting verification.")
            await activities.save_state(state)
            return VerificationStartEvent()

        # Send an event for EACH active worker
        for worker in active_workers:
            ctx.send_event(WorkerStartEvent(worker_state=worker))

        # We return None because we used send_event to emit multiple events
        return None

    @step(num_workers=10)
    async def execute_worker(
        self, ctx: Context, ev: WorkerStartEvent
    ) -> WorkerResultEvent:
        """
        Executes a single worker's iteration in parallel.
        """
        # Execute the activity (IO bound)
        # This calls the implementation in activities.py
        result = await activities.execute_worker_iteration(ev.worker_state)

        return WorkerResultEvent(
            worker_id=ev.worker_state.id,
            pages_fetched=result.get("pages_fetched", 0),
            entities_found=result.get("entities_found", 0),
            new_entities=result.get("new_entities", 0),
            novelty_rate=result.get("novelty_rate", 0.0),
            status=result.get("status", "PRODUCTIVE"),
            extracted_data=result.get("extracted_data", []),
            discovered_links=result.get("discovered_links", []),
        )

    @step
    async def aggregate(
        self, ctx: Context, ev: WorkerResultEvent
    ) -> Optional[Union[IterationCompleteEvent, StopEvent]]:
        """
        Fan-In: Aggregates results from all workers, checks stopping criteria, and updates plan.
        """
        state: ResearchState = await ctx.store.get("state")
        active_worker_count = len(
            [
                w
                for w in state.workers.values()
                if w.status in ["ACTIVE", "PRODUCTIVE", "DECLINING"]
            ]
        )

        # Wait for ALL active workers to return results
        events = ctx.collect_events(ev, [WorkerResultEvent] * active_worker_count)
        if events is None:
            return None

        # All workers finished this iteration
        worker_results: List[WorkerResultEvent] = events

        # --- Critical Section: State Update ---
        # Update metrics and worker status based on results

        total_new_entities = 0
        total_pages = 0

        for res in worker_results:
            w_state = state.workers.get(res.worker_id)
            if w_state:
                w_state.pages_fetched += res.pages_fetched
                w_state.entities_found += res.entities_found
                w_state.new_entities += res.new_entities
                w_state.status = res.status
                total_new_entities += res.new_entities
                total_pages += res.pages_fetched

                # Update Personal Queue
                # 1. Remove consumed URLs (FIFO)
                consumed_urls = getattr(res, "consumed_urls", [])
                # Filter out consumed URLs from the personal queue
                # Note: This simple filtering assumes unique URLs in queue or FIFO consistency.
                # A more robust approach might be to pop N items if we knew exactly N were popped from the front.
                # Given logic in activity: `url_queue.append(worker_state.personal_queue.pop(0))`,
                # the activity effectively consumed the *head* of the queue passed to it.
                # BUT: The activity operates on a *copy* of the state.
                # So we simply need to remove the URLs that were in the queue.

                # However, since `consumed_urls` includes both search results AND personal queue items,
                # we should only remove those that were actually IN the personal queue.
                # Optimization: Rebuild queue excluding consumed items.
                if consumed_urls:
                    w_state.personal_queue = [
                        url
                        for url in w_state.personal_queue
                        if url not in consumed_urls
                    ]

                # 2. Add Discovered Links
                for link in getattr(res, "discovered_links", []):
                    if link not in state.visited_urls:
                        state.visited_urls.add(link)
                        w_state.personal_queue.append(link)

            # Merge entities into global state
            for item in res.extracted_data:
                canonical = item["canonical"]
                alias = item["alias"]
                evidence = item["evidence"]

                # Extract structured attributes
                attributes = {
                    "target": item.get("target"),
                    "modality": item.get("modality"),
                    "product_stage": item.get("product_stage"),
                    "indication": item.get("indication"),
                    "geography": item.get("geography"),
                    "owner": item.get("owner"),
                    **item.get("attributes", {})  # Merge any other attributes if present
                }
                # Remove None values
                attributes = {k: v for k, v in attributes.items() if v}

                if canonical not in state.known_entities:
                    state.known_entities[canonical] = Entity(
                        canonical_name=canonical,
                        mention_count=1,
                        drug_class=attributes.get("modality"), # specific field on Entity
                        clinical_phase=attributes.get("product_stage"), # specific field on Entity
                        attributes=attributes # generic dict for everything else
                    )
                else:
                    entity = state.known_entities[canonical]
                    entity.mention_count += 1
                    
                    # Merge new attributes into existing
                    current_attrs = entity.attributes or {}
                    for k, v in attributes.items():
                        # Update if missing or Unknown
                        if v and v != "Unknown":
                            if k not in current_attrs or current_attrs[k] == "Unknown":
                                current_attrs[k] = v
                    
                    entity.attributes = current_attrs
                    
                    # Update specific fields if improved
                    if attributes.get("modality") and attributes["modality"] != "Unknown":
                        entity.drug_class = attributes["modality"]
                    if attributes.get("product_stage") and attributes["product_stage"] != "Unknown":
                        entity.clinical_phase = attributes["product_stage"]

                # Add evidence and aliases
                entity = state.known_entities[canonical]
                entity.aliases.add(alias)
                entity.evidence.extend(evidence)

        state.iteration_count += 1
        global_novelty = total_new_entities / max(total_pages, 1)
        log_msg = (
            "Iteration %s completed. Found %d new entities. Novelty Rate: %.2f entities/page"
        )
        state.logs.append(
            log_msg % (state.iteration_count, total_new_entities, global_novelty)
        )
        logger = get_session_logger(state.id)
        logger.info(
            log_msg,
            state.iteration_count,
            total_new_entities,
            global_novelty,
        )

        # --- Stopping Criteria Check ---
        # 1. Budget exhausted? (Stub check)
        # 2. Novelty low? (Stop if novelty < 5% for 2 iterations)
        # 3. Max iterations?
        max_iters = int(os.getenv("MAX_ITERATIONS", "5"))

        should_stop = False
        if state.iteration_count >= max_iters:
            should_stop = True
            msg = "Stopping: Max iterations reached."
            state.logs.append(msg)
            logger.info(msg)

        if global_novelty < 0.05 and state.iteration_count > 1:
            should_stop = True
            msg = f"Stopping: Low novelty detected ({global_novelty:.2%})."
            state.logs.append(msg)
            logger.info(msg)

        # 4. All workers dead?
        all_dead = all(w.status == "DEAD_END" for w in state.workers.values())
        if all_dead and state.workers:
            should_stop = True
            msg = "Stopping: All workers reached dead ends."
            state.logs.append(msg)
            logger.info(msg)

        if should_stop:
            state.status = "verification_pending"
            await activities.save_state(state)
            return VerificationStartEvent()

        # Iterate Planning (Adaptive Step)
        # Call the agentic update logic to analyze discoveries and make strategic decisions
        new_plan = await activities.update_plan(state)
        state.plan = new_plan

        # Handle worker kills
        for worker_id in new_plan.workers_to_kill:
            if worker_id in state.workers:
                state.workers[worker_id].status = "DEAD_END"
                msg = f"Killed worker {worker_id} (exhausted)"
                state.logs.append(msg)
                logger.info(msg)

        # Handle worker spawns
        for worker_cfg in new_plan.initial_workers:
            if worker_cfg.worker_id not in state.workers:
                # New worker spawned by adaptive planning
                w_state = WorkerState(
                    id=worker_cfg.worker_id,
                    research_id=state.id,
                    strategy=worker_cfg.strategy,
                    queries=worker_cfg.example_queries,
                    status="ACTIVE",
                )
                state.workers[w_state.id] = w_state
                msg = f"Spawned new worker {w_state.id} with strategy: {worker_cfg.strategy}"
                state.logs.append(msg)
                logger.info(msg)

        # Update queries for existing workers
        for worker_id, new_queries in new_plan.updated_queries.items():
            if worker_id in state.workers:
                state.workers[worker_id].queries = new_queries
                msg = f"Updated queries for worker {worker_id}: {len(new_queries)} new queries"
                state.logs.append(msg)
                logger.info(msg)

        await activities.save_state(state)

        return IterationCompleteEvent(
            iteration=state.iteration_count,
            summary=f"Finished iteration {state.iteration_count}",
            global_novelty_rate=global_novelty,
        )

    @step
    async def dispatch_verification(
        self, ctx: Context, ev: VerificationStartEvent
    ) -> Optional[VerifyEntityEvent]:
        """
        Dispatches verification tasks for all known entities.
        """
        state: ResearchState = await ctx.store.get("state")
        logger = get_session_logger(state.id)
        logger.info("Starting verification phase for %d entities.", len(state.known_entities))
        state.logs.append(f"Starting verification phase for {len(state.known_entities)} entities.")
        
        # Constraints from the plan
        constraints = state.plan.query_analysis
        
        # Send event for each entity
        for entity in state.known_entities.values():
            # Only verify unverified or uncertain entities? For now, verify all.
            # Convert entity to dict for event payload
            entity_dict = entity.model_dump()
            ctx.send_event(VerifyEntityEvent(entity=entity_dict, constraints=constraints))
            
        return None

    @step(num_workers=10)
    async def execute_verification(
        self, ctx: Context, ev: VerifyEntityEvent
    ) -> VerificationResultEvent:
        """
        Executes verification for a single entity using the VerificationAgent.
        """
        # Call activity
        result = await activities.verify_entity(ev.entity, ev.constraints)
        return VerificationResultEvent(result=result)

    @step
    async def aggregate_verification(
        self, ctx: Context, ev: VerificationResultEvent
    ) -> StopEvent:
        """
        Aggregates verification results and updates state.
        """
        state: ResearchState = await ctx.store.get("state")
        num_entities = len(state.known_entities)
        
        # Wait for all verification results
        events = ctx.collect_events(ev, [VerificationResultEvent] * num_entities)
        if events is None:
            return None
            
        logger = get_session_logger(state.id)
        logger.info("All verification tasks completed.")
        
        # Update state with results
        verified_count = 0
        rejected_count = 0
        uncertain_count = 0
        
        for res_ev in events:
            res = res_ev.result
            canonical = res.get("canonical_name")
            if canonical and canonical in state.known_entities:
                ent = state.known_entities[canonical]
                ent.verification_status = res.get("status", "UNCERTAIN")
                ent.rejection_reason = res.get("rejection_reason")
                ent.confidence_score = res.get("confidence", 0.0)
                
                if ent.verification_status == "VERIFIED":
                    verified_count += 1
                elif ent.verification_status == "REJECTED":
                    rejected_count += 1
                else:
                    uncertain_count += 1

        gap_events_dispatched = 0
        if uncertain_count > 0:
            logger.info("Found %d uncertain entities. Triggering gap analysis.", uncertain_count)
            
            # Identify gaps and dispatch events
            for res_ev in events:
                res = res_ev.result
                if res.get("status") == "UNCERTAIN":
                    ent_data = state.known_entities[res["canonical_name"]].model_dump()
                    queries = await activities.analyze_gaps(ent_data, res)
                    if queries:
                       ctx.send_event(GapFillEvent(entity=ent_data, queries=queries))
                       gap_events_dispatched += 1

        summary_msg = (
            f"Verification Complete: {verified_count} Verified, "
            f"{rejected_count} Rejected, {uncertain_count} Uncertain."
        )
        state.logs.append(summary_msg)
        logger.info(summary_msg)
        
        await activities.save_state(state)
        
        if gap_events_dispatched > 0:
            logger.info("Dispatching %d gap-filling tasks.", gap_events_dispatched)
            return GapFillingStartEvent(count=gap_events_dispatched)
        
        return StopEvent(result=state)

    @step
    async def execute_gap_fill(self, ctx: Context, ev: GapFillEvent) -> GapFilledEvent:
        """
        Executes gap-filling search for a specific entity.
        Reuses the existing worker execution logic but with targeted queries.
        """
        # Create a temporary worker state for this specific task
        worker_id = f"gap-fill-{ev.entity.get('canonical_name')}"[:30]
        # Clean ID
        worker_id = re.sub(r"[^a-zA-Z0-9-]", "", worker_id)
        
        state: ResearchState = await ctx.store.get("state")
        
        # Configure a targeted worker
        worker = WorkerState(
            id=worker_id,
            research_id=state.id,
            strategy="gap_filling",
            queries=ev.queries,
            status="ACTIVE",
            personal_queue=[], # Could seed with entity domains if known
        )
        
        # Execute single iteration (search -> fetch -> extract)
        # We reuse the existing activity!
        await activities.execute_worker_iteration(worker)
        
        return GapFilledEvent(worker_id=worker_id)

    @step
    async def await_gap_filling(self, ctx: Context, ev: GapFillingStartEvent) -> StopEvent:
        """
        Waits for all gap-filling tasks to complete.
        """
        state: ResearchState = await ctx.store.get("state")
        logger = get_session_logger(state.id)
        
        logger.info("Waiting for %d gap-filling tasks...", ev.count)
        
        # Wait for all gap filled events
        # Note: We don't strictly need the results payload as execute_worker_iteration saves to DB side-effect
        events = ctx.collect_events(ev, [GapFilledEvent] * ev.count)
        if events is None:
            return None
            
        logger.info("All gap-filling tasks completed.")
        state.logs.append("Gap filling phase complete.")
        state.status = "completed"
        
        await activities.save_state(state)
        return StopEvent(result=state)
