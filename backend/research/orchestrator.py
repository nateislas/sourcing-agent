import os
from typing import List, Optional, Union

from llama_index.core.workflow import (
    Workflow,
    Context,
    StartEvent,
    StopEvent,
    step,
)

from backend.research.state import ResearchState, WorkerState, Entity
from backend.research.events import (
    PlanCreatedEvent,
    WorkerStartEvent,
    WorkerResultEvent,
    IterationCompleteEvent,
)
from backend.research import activities
from backend.research.logging_utils import get_session_logger


class DeepResearchWorkflow(Workflow):
    """
    Orchestrates the iterative research process.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @step
    async def start(self, ctx: Context, ev: StartEvent) -> PlanCreatedEvent:
        """
        Initializes the research state and generates the initial plan.
        """
        topic = ev.get("topic")
        if not topic:
            raise ValueError("Topic is required to start research.")

        # Initialize global state in Context
        state = ResearchState(topic=topic, status="running")
        # Save initial state to Context (and potentially DB)
        await ctx.store.set("state", state)

        # Setup session logger
        logger = get_session_logger(state.id)
        logger.info(f"Research session started for topic: {topic}")

        # Log start
        state.logs.append("Workflow initialized.")
        await activities.save_state(state)

        # Generate Initial Plan (Agentic Step)
        plan = await activities.generate_initial_plan(topic, research_id=state.id)
        state.plan = plan
        state.logs.append(f"Plan generated: {plan.current_hypothesis}")

        # Initialize workers from plan
        for worker_cfg in plan.initial_workers:
            # Create WorkerState objects (ID preserved if provided, or generated by default)
            w_state = WorkerState(
                id=worker_cfg.worker_id,
                research_id=state.id,
                strategy=worker_cfg.strategy,
                queries=worker_cfg.example_queries,  # Pass actual queries
                status="ACTIVE",
            )
            state.workers[w_state.id] = w_state

        await activities.save_state(state)

        return PlanCreatedEvent(plan=plan)

    @step
    async def dispatch(
        self, ctx: Context, ev: Union[PlanCreatedEvent, IterationCompleteEvent]
    ) -> Optional[WorkerStartEvent]:
        """
        Fan-Out: Dispatches work to all active workers.
        """
        state: ResearchState = await ctx.store.get("state")

        # Identify active workers (including those categorized as productive or declining)
        active_workers = [
            w
            for w in state.workers.values()
            if w.status in ["ACTIVE", "PRODUCTIVE", "DECLINING"]
        ]

        if not active_workers:
            # No viable workers left to dispatch. Transition to completed.
            state.status = "completed"
            state.logs.append("No active or productive workers remaining. Stopping.")
            await activities.save_state(state)
            return StopEvent(result=state)

        # Send an event for EACH active worker
        for worker in active_workers:
            ctx.send_event(WorkerStartEvent(worker_state=worker))

        # We return None because we used send_event to emit multiple events
        return None

    @step(num_workers=10)
    async def execute_worker(
        self, ctx: Context, ev: WorkerStartEvent
    ) -> WorkerResultEvent:
        """
        Executes a single worker's iteration in parallel.
        """
        # Execute the activity (IO bound)
        # This calls the implementation in activities.py
        result = await activities.execute_worker_iteration(ev.worker_state)

        return WorkerResultEvent(
            worker_id=ev.worker_state.id,
            pages_fetched=result.get("pages_fetched", 0),
            entities_found=result.get("entities_found", 0),
            new_entities=result.get("new_entities", 0),
            novelty_rate=result.get("novelty_rate", 0.0),
            status=result.get("status", "PRODUCTIVE"),
            extracted_data=result.get("extracted_data", []),
            discovered_links=result.get("discovered_links", []),
        )

    @step
    async def aggregate(
        self, ctx: Context, ev: WorkerResultEvent
    ) -> Optional[Union[IterationCompleteEvent, StopEvent]]:
        """
        Fan-In: Aggregates results from all workers, checks stopping criteria, and updates plan.
        """
        state: ResearchState = await ctx.store.get("state")
        active_worker_count = len(
            [
                w
                for w in state.workers.values()
                if w.status in ["ACTIVE", "PRODUCTIVE", "DECLINING"]
            ]
        )

        # Wait for ALL active workers to return results
        events = ctx.collect_events(ev, [WorkerResultEvent] * active_worker_count)
        if events is None:
            return None

        # All workers finished this iteration
        worker_results: List[WorkerResultEvent] = events

        # --- Critical Section: State Update ---
        # Update metrics and worker status based on results

        total_new_entities = 0
        total_pages = 0

        for res in worker_results:
            w_state = state.workers.get(res.worker_id)
            if w_state:
                w_state.pages_fetched += res.pages_fetched
                w_state.entities_found += res.entities_found
                w_state.new_entities += res.new_entities
                w_state.status = res.status
                total_new_entities += res.new_entities
                total_pages += res.pages_fetched

            # Process Discovered Links
            for link in getattr(res, "discovered_links", []):
                if link not in state.visited_urls:
                    state.visited_urls.add(link)
                    w_state.personal_queue.append(link)

            # Merge entities into global state
            for item in res.extracted_data:
                canonical = item["canonical"]
                alias = item["alias"]
                evidence = item["evidence"]

                if canonical not in state.known_entities:
                    state.known_entities[canonical] = Entity(
                        canonical_name=canonical,
                        mention_count=1,
                        drug_class=item.get("drug_class"),
                        clinical_phase=item.get("clinical_phase"),
                    )
                else:
                    entity = state.known_entities[canonical]
                    entity.mention_count += 1
                    # Update metadata if missing
                    if not entity.drug_class:
                        entity.drug_class = item.get("drug_class")
                    if not entity.clinical_phase:
                        entity.clinical_phase = item.get("clinical_phase")

                # Add evidence and aliases
                entity = state.known_entities[canonical]
                entity.aliases.add(alias)
                entity.evidence.extend(evidence)

        state.iteration_count += 1
        global_novelty = total_new_entities / max(total_pages, 1)
        log_msg = f"Iteration {state.iteration_count} completed. Found {total_new_entities} new entities. Global Novelty: {global_novelty:.2%}"
        state.logs.append(log_msg)
        logger = get_session_logger(state.id)
        logger.info(log_msg)

        # --- Stopping Criteria Check ---
        # 1. Budget exhausted? (Stub check)
        # 2. Novelty low? (Stop if novelty < 5% for 2 iterations)
        # 3. Max iterations?
        max_iters = int(os.getenv("MAX_ITERATIONS", 5))

        should_stop = False
        if state.iteration_count >= max_iters:
            should_stop = True
            msg = "Stopping: Max iterations reached."
            state.logs.append(msg)
            logger.info(msg)

        if global_novelty < 0.05 and state.iteration_count > 1:
            should_stop = True
            msg = f"Stopping: Low novelty detected ({global_novelty:.2%})."
            state.logs.append(msg)
            logger.info(msg)

        # 4. All workers dead?
        all_dead = all(w.status == "DEAD_END" for w in state.workers.values())
        if all_dead and state.workers:
            should_stop = True
            msg = "Stopping: All workers reached dead ends."
            state.logs.append(msg)
            logger.info(msg)

        if should_stop:
            state.status = "completed"
            await activities.save_state(state)
            return StopEvent(result=state)

        # Iterate Planning (Adaptive Step)
        # Call the agentic update logic to analyze discoveries and make strategic decisions
        new_plan = await activities.update_plan(state)
        state.plan = new_plan

        # Handle worker kills
        for worker_id in new_plan.workers_to_kill:
            if worker_id in state.workers:
                state.workers[worker_id].status = "DEAD_END"
                msg = f"Killed worker {worker_id} (exhausted)"
                state.logs.append(msg)
                logger.info(msg)

        # Handle worker spawns
        for worker_cfg in new_plan.initial_workers:
            if worker_cfg.worker_id not in state.workers:
                # New worker spawned by adaptive planning
                w_state = WorkerState(
                    id=worker_cfg.worker_id,
                    research_id=state.id,
                    strategy=worker_cfg.strategy,
                    queries=worker_cfg.example_queries,
                    status="ACTIVE",
                )
                state.workers[w_state.id] = w_state
                msg = f"Spawned new worker {w_state.id} with strategy: {worker_cfg.strategy}"
                state.logs.append(msg)
                logger.info(msg)

        # Update queries for existing workers
        for worker_id, new_queries in new_plan.updated_queries.items():
            if worker_id in state.workers:
                state.workers[worker_id].queries = new_queries
                msg = f"Updated queries for worker {worker_id}: {len(new_queries)} new queries"
                state.logs.append(msg)
                logger.info(msg)

        await activities.save_state(state)

        return IterationCompleteEvent(
            iteration=state.iteration_count,
            summary=f"Finished iteration {state.iteration_count}",
            global_novelty_rate=global_novelty,
        )
